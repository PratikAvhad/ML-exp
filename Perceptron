import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import datasets

# Unit step activation function
def unit_step_func(x):
    return np.where(x > 0 , 1, 0)  # Returns 1 if x > 0, otherwise 0

# Perceptron class definition
class Perceptron:

    def __init__(self, learning_rate=0.01, n_iters=1000):
        # Initialize model parameters: learning rate, number of iterations, weights, and bias
        self.lr = learning_rate
        self.n_iters = n_iters
        self.activation_func = unit_step_func
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        # Train the Perceptron on the input data (X) and target labels (y)
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)  # Initialize weights
        self.bias = 0  # Initialize bias

        y_ = np.where(y > 0 , 1, 0)  # Convert labels to binary (1 or 0)

        for _ in range(self.n_iters):  # Iterate for n_iters epochs
            for idx, x_i in enumerate(X):  # Loop through each training sample
                linear_output = np.dot(x_i, self.weights) + self.bias  # Calculate the linear output
                y_predicted = self.activation_func(linear_output)  # Apply activation function

                # Perceptron weight update rule
                update = self.lr * (y_[idx] - y_predicted)  # Calculate the weight update
                self.weights += update * x_i  # Update the weights
                self.bias += update  # Update the bias

    def predict(self, X):
        # Make predictions based on input data X
        linear_output = np.dot(X, self.weights) + self.bias  # Calculate linear output for each sample
        y_predicted = self.activation_func(linear_output)  # Apply activation function to output
        return y_predicted

# Main section: dataset creation, model training, and evaluation
if __name__ == "__main__":

    # Accuracy function to evaluate model performance
    def accuracy(y_true, y_pred):
        accuracy = np.sum(y_true == y_pred) / len(y_true)  # Calculate accuracy as the ratio of correct predictions
        return accuracy

    # Create synthetic dataset with 2 features and 2 classes
    X, y = datasets.make_blobs(n_samples=150, n_features=2, centers=2, cluster_std=1.05, random_state=2)
    # Split dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

    # Initialize and train Perceptron model
    p = Perceptron(learning_rate=0.01, n_iters=1000)
    p.fit(X_train, y_train)
    # Make predictions on the test set
    predictions = p.predict(X_test)

    # Print classification accuracy
    print("Perceptron classification accuracy", accuracy(y_test, predictions))

    # Plotting the decision boundary and data points
    def plot_decision_boundary(X, y, model):
        # Create a grid of points to cover the feature space
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                             np.arange(y_min, y_max, 0.1))

        # Get predictions for each point in the grid
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        # Plot the contour (decision boundary)
        plt.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.coolwarm)

        # Plot the data points
        plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.title('Perceptron Decision Boundary')
        plt.show()

    # Plot the decision boundary on the training data
    plot_decision_boundary(X_train, y_train, p)
